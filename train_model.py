"""
Fine-tune Sentence Transformer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Movie Recommendation System
‡πÉ‡∏ä‡πâ Contrastive Learning ‡πÅ‡∏•‡∏∞ Triplet Loss

‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:
1. ‡∏™‡∏£‡πâ‡∏≤‡∏á training pairs ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏±‡∏á (positive/negative pairs)
2. Fine-tune all-mpnet-base-v2 model
3. Evaluate ‡∏î‡πâ‡∏ß‡∏¢ similarity metrics
4. Save fine-tuned model ‡πÅ‡∏•‡∏∞ embeddings
"""

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import random
import os
from datetime import datetime
import json
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random seed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reproducibility
random.seed(42)
np.random.seed(42)

print("=" * 80)
print("üéì Fine-tuning Movie Recommendation Model")
print("=" * 80)

# ========================
# 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ========================
print("\nüìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
movies_df = pd.read_pickle('data/movies.pkl')
print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(movies_df):,} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô train/validation/test
train_df, temp_df = train_test_split(movies_df, test_size=0.3, random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

print(f"üì¶ Train: {len(train_df):,} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
print(f"üì¶ Validation: {len(val_df):,} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
print(f"üì¶ Test: {len(test_df):,} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")


# ========================
# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Pairs
# ========================
def create_positive_pair(df, row):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Positive Pair (‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô) ‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤:
    - ‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡∏±‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (genres overlap)
    - ‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    - ‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    - ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    """
    candidate_scores = []
    
    for idx, other_row in df.iterrows():
        if idx == row.name:
            continue
        
        score = 0.0
        
        # Genre overlap (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
        genre_overlap = len(set(row['genres']) & set(other_row['genres']))
        score += genre_overlap * 0.4
        
        # ‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        if row['director'] and other_row['director'] and row['director'] == other_row['director']:
            score += 0.3
        
        # ‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        cast_overlap = len(set(row['cast'][:5]) & set(other_row['cast'][:5]))
        score += cast_overlap * 0.1
        
        # ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        keyword_overlap = len(set(row['keywords'][:10]) & set(other_row['keywords'][:10]))
        score += keyword_overlap * 0.05
        
        if score > 0.3:  # ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£
            candidate_scores.append((idx, score))
    
    if candidate_scores:
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        candidate_scores.sort(key=lambda x: x[1], reverse=True)
        selected_idx = candidate_scores[0][0]
        return df.loc[selected_idx], candidate_scores[0][1]
    
    return None, 0.0


def create_negative_pair(df, row):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Negative Pair (‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô):
    - ‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏¢‡∏™‡∏¥‡πâ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á
    - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á/‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    """
    candidates = df[~df['genres'].apply(
        lambda x: bool(set(x) & set(row['genres']))
    )]
    
    if len(candidates) > 0:
        return candidates.sample(1).iloc[0]
    
    return None


def create_training_examples(df, num_pairs=10000, pair_type='both'):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á InputExample ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training
    
    Args:
        df: DataFrame ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏á
        num_pairs: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô pairs ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        pair_type: 'positive', 'negative', 'both'
    """
    examples = []
    
    print(f"\nüîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á {num_pairs} training pairs...")
    
    sampled_movies = df.sample(min(num_pairs, len(df)), random_state=42)
    
    for idx, (_, row) in enumerate(sampled_movies.iterrows()):
        if idx % 1000 == 0:
            print(f"  Progress: {idx}/{num_pairs}")
        
        # Positive pair
        if pair_type in ['positive', 'both']:
            pos_movie, similarity = create_positive_pair(df, row)
            if pos_movie is not None:
                examples.append(InputExample(
                    texts=[row['rich_description'], pos_movie['rich_description']],
                    label=min(similarity, 1.0)  # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ 0-1
                ))
        
        # Negative pair
        if pair_type in ['negative', 'both']:
            neg_movie = create_negative_pair(df, row)
            if neg_movie is not None:
                examples.append(InputExample(
                    texts=[row['rich_description'], neg_movie['rich_description']],
                    label=0.0  # ‡πÑ‡∏°‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏¢
                ))
    
    print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à: {len(examples)} pairs")
    return examples


print("\nüéØ ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Examples...")
train_examples = create_training_examples(train_df, num_pairs=8000, pair_type='both')

print("\nüéØ ‡∏™‡∏£‡πâ‡∏≤‡∏á Validation Examples...")
val_examples = create_training_examples(val_df, num_pairs=1500, pair_type='both')

print("\nüéØ ‡∏™‡∏£‡πâ‡∏≤‡∏á Test Examples...")
test_examples = create_training_examples(test_df, num_pairs=1000, pair_type='both')


# ========================
# 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° DataLoader
# ========================
print("\nüì¶ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° DataLoader...")
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)
print(f"  Train batches: {len(train_dataloader)}")


# ========================
# 4. ‡πÇ‡∏´‡∏•‡∏î Base Model
# ========================
print("\nüß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î base model...")
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"üì± ‡πÉ‡∏ä‡πâ device: {device}")
model = SentenceTransformer('BAAI/bge-base-en-v1.5', device=device)
print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î BAAI/bge-base-en-v1.5 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (SOTA Model)")

# ‡∏•‡πâ‡∏≤‡∏á cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM
if device == "mps":
    torch.mps.empty_cache()


# ========================
# 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Evaluator
# ========================
print("\nüß™ ‡∏™‡∏£‡πâ‡∏≤‡∏á Evaluators...")

# ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà Embedding Similarity Evaluator ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM
val_sentences1 = [ex.texts[0] for ex in val_examples[:600]]
val_sentences2 = [ex.texts[1] for ex in val_examples[:600]]
val_scores = [ex.label for ex in val_examples[:600]]

evaluator = evaluation.EmbeddingSimilarityEvaluator(
    val_sentences1,
    val_sentences2,
    val_scores,
    name='movie-validation'
)

print("‚úÖ ‡πÉ‡∏ä‡πâ EmbeddingSimilarityEvaluator (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM)")


# ========================
# 6. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Loss Function
# ========================
print("\n‚öôÔ∏è  ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Loss Function...")
train_loss = losses.CosineSimilarityLoss(model)
print("‚úÖ ‡πÉ‡∏ä‡πâ CosineSimilarityLoss")


# ========================
# 7. Fine-tuning!
# ========================
output_path = 'models/movie-mpnet-finetuned'
os.makedirs(output_path, exist_ok=True)

print("\n" + "=" * 80)
print("üî• ‡πÄ‡∏£‡∏¥‡πà‡∏° Fine-tuning...")
print("=" * 80)

training_config = {
    'base_model': 'all-mpnet-base-v2',
    'train_examples': len(train_examples),
    'val_examples': len(val_examples),
    'batch_size': 32,
    'epochs': 4,
    'warmup_steps': 50,
    'evaluation_steps': 400,
    'save_best_model': True,
    'device': device,
    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å config
with open(f'{output_path}/training_config.json', 'w', encoding='utf-8') as f:
    json.dump(training_config, f, indent=2, ensure_ascii=False)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=4,
    warmup_steps=50,
    evaluator=evaluator,
    evaluation_steps=400,
    output_path=output_path,
    save_best_model=True,
    show_progress_bar=True
)

print("\n‚úÖ Fine-tuning ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")

# ‡∏•‡πâ‡∏≤‡∏á cache ‡∏´‡∏•‡∏±‡∏á training
if device == "mps":
    torch.mps.empty_cache()


# ========================
# 8. Evaluation ‡∏ö‡∏ô Test Set
# ========================
print("\n" + "=" * 80)
print("üìä Evaluating ‡∏ö‡∏ô Test Set...")
print("=" * 80)

# ‡πÇ‡∏´‡∏•‡∏î best model
best_model = SentenceTransformer(output_path)

# Test Embedding Similarity
test_sentences1 = [ex.texts[0] for ex in test_examples[:1000]]
test_sentences2 = [ex.texts[1] for ex in test_examples[:1000]]
test_scores = [ex.label for ex in test_examples[:1000]]

test_evaluator = evaluation.EmbeddingSimilarityEvaluator(
    test_sentences1,
    test_sentences2,
    test_scores,
    name='movie-test'
)

print("\nüß™ Test Set Results:")
test_result = test_evaluator(best_model, output_path=output_path)

# ========================
# 9. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Pairwise Metrics ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü
# ========================
print("\n" + "=" * 80)
print("üìä ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Pairwise Metrics ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü...")
print("=" * 80)

def compute_pair_metrics(model, examples, threshold=0.5, max_samples=2000):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Precision, Recall, F1, Accuracy, Spearman
    """
    subset = examples[:max_samples]
    t1 = [e.texts[0] for e in subset]
    t2 = [e.texts[1] for e in subset]
    
    print(f"  Encoding {len(t1)} pairs...")
    emb1 = model.encode(t1, batch_size=16, normalize_embeddings=True, show_progress_bar=True, device=device)
    emb2 = model.encode(t2, batch_size=16, normalize_embeddings=True, show_progress_bar=False, device=device)
    
    sims = [float(np.dot(a, b)) for a, b in zip(emb1, emb2)]
    labels = [float(e.label) for e in subset]
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô binary classification (threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö label ‡πÅ‡∏•‡∏∞ similarity)
    true_bin = [1 if l >= 0.3 else 0 for l in labels]
    pred_bin = [1 if s >= threshold else 0 for s in sims]
    
    tp = sum(1 for t, p in zip(true_bin, pred_bin) if t == 1 and p == 1)
    fp = sum(1 for t, p in zip(true_bin, pred_bin) if t == 0 and p == 1)
    fn = sum(1 for t, p in zip(true_bin, pred_bin) if t == 1 and p == 0)
    tn = sum(1 for t, p in zip(true_bin, pred_bin) if t == 0 and p == 0)
    
    precision = tp / (tp + fp + 1e-9)
    recall = tp / (tp + fn + 1e-9)
    f1 = 2 * precision * recall / (precision + recall + 1e-9)
    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-9)
    
    spearman_corr, _ = spearmanr(labels, sims)
    
    return {
        "threshold": threshold,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy,
        "spearman": float(spearman_corr),
        "sims": sims,
        "labels": labels,
        "confusion_matrix": {"tp": tp, "fp": fp, "fn": fn, "tn": tn}
    }

print("\nüìà Fine-tuned Model Metrics:")
pair_metrics = compute_pair_metrics(best_model, test_examples, threshold=0.5, max_samples=2000)

# ‡∏•‡πâ‡∏≤‡∏á cache
if device == "mps":
    torch.mps.empty_cache()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü
metrics_dir = f"{output_path}/metrics"
os.makedirs(metrics_dir, exist_ok=True)

# ‡∏Å‡∏£‡∏≤‡∏ü 1: Similarity Distribution
print("\nüìä ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü Similarity Distribution...")
plt.figure(figsize=(10, 6))
sns.histplot(pair_metrics["sims"], bins=50, kde=True, color="#2563eb")
plt.axvline(0.5, color="red", linestyle="--", linewidth=2, label="Threshold 0.5")
plt.title("Cosine Similarity Distribution (Fine-tuned Model)", fontsize=14, fontweight='bold')
plt.xlabel("Cosine Similarity", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.legend(fontsize=10)
plt.grid(alpha=0.3)
dist_path = f"{metrics_dir}/similarity_distribution.png"
plt.tight_layout()
plt.savefig(dist_path, dpi=150)
plt.close()
print(f"  ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {dist_path}")

# ‡∏Å‡∏£‡∏≤‡∏ü 2: Label vs Similarity Scatter
print("\nüìä ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü Label vs Similarity...")
plt.figure(figsize=(10, 6))
sns.scatterplot(x=pair_metrics["labels"], y=pair_metrics["sims"], s=20, alpha=0.4, color="#10b981")
plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label="Perfect Correlation")
plt.title("Ground Truth Label vs Predicted Similarity", fontsize=14, fontweight='bold')
plt.xlabel("Ground Truth Label (0-1)", fontsize=12)
plt.ylabel("Cosine Similarity", fontsize=12)
plt.legend(fontsize=10)
plt.grid(alpha=0.3)
scatter_path = f"{metrics_dir}/label_vs_similarity.png"
plt.tight_layout()
plt.savefig(scatter_path, dpi=150)
plt.close()
print(f"  ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {scatter_path}")

# ‡∏Å‡∏£‡∏≤‡∏ü 3: Confusion Matrix
print("\nüìä ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü Confusion Matrix...")
cm = pair_metrics["confusion_matrix"]
cm_array = np.array([[cm["tn"], cm["fp"]], [cm["fn"], cm["tp"]]])

plt.figure(figsize=(8, 6))
sns.heatmap(cm_array, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'],
            cbar_kws={'label': 'Count'})
plt.title("Confusion Matrix (Threshold=0.5)", fontsize=14, fontweight='bold')
plt.ylabel("True Label", fontsize=12)
plt.xlabel("Predicted Label", fontsize=12)
cm_path = f"{metrics_dir}/confusion_matrix.png"
plt.tight_layout()
plt.savefig(cm_path, dpi=150)
plt.close()
print(f"  ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {cm_path}")

print("\nüìà Pairwise Classification Metrics:")
print(f"  Threshold:  {pair_metrics['threshold']}")
print(f"  Precision:  {pair_metrics['precision']:.4f}")
print(f"  Recall:     {pair_metrics['recall']:.4f}")
print(f"  F1 Score:   {pair_metrics['f1']:.4f}")
print(f"  Accuracy:   {pair_metrics['accuracy']:.4f}")
print(f"  Spearman:   {pair_metrics['spearman']:.4f}")
print(f"\n  Confusion Matrix:")
print(f"    TP: {cm['tp']}, FP: {cm['fp']}")
print(f"    FN: {cm['fn']}, TN: {cm['tn']}")


# ========================
# 10. ‡∏™‡∏£‡πâ‡∏≤‡∏á Embeddings ‡πÉ‡∏´‡∏°‡πà
# ========================
print("\n" + "=" * 80)
print("üßÆ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...")
print("=" * 80)

all_descriptions = movies_df['rich_description'].tolist()
new_embeddings = best_model.encode(
    all_descriptions,
    show_progress_bar=True,
    batch_size=32,
    normalize_embeddings=True
)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å embeddings
embedding_path = 'data/movie_embeddings_finetuned.npy'
np.save(embedding_path, new_embeddings)
print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å embeddings ‡∏ó‡∏µ‡πà: {embedding_path}")
print(f"   Shape: {new_embeddings.shape}")


# ========================
# 10. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Original vs Fine-tuned
# ========================
print("\n" + "=" * 80)
print("üìà ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Performance: Original vs Fine-tuned")
print("=" * 80)

# ‡πÇ‡∏´‡∏•‡∏î original model
original_model = SentenceTransformer('BAAI/bge-base-en-v1.5')

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö test set
print("\nüîµ Original Model (BAAI/bge-base-en-v1.5 - Base):")
original_result = test_evaluator(original_model)
original_score = original_result if isinstance(original_result, float) else original_result.get('spearman_cosine', 0.0)

print("\nüü¢ Fine-tuned Model:")
finetuned_result = test_evaluator(best_model)
finetuned_score = finetuned_result if isinstance(finetuned_result, float) else finetuned_result.get('spearman_cosine', 0.0)

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
improvement = {
    'spearman_correlation': finetuned_score - original_score
}

print("\nüìä Summary:")
print(f"  Original Spearman: {original_score:.4f}")
print(f"  Fine-tuned Spearman: {finetuned_score:.4f}")
print(f"  Improvement: {improvement['spearman_correlation']:.4f} ({improvement['spearman_correlation']*100:.2f}%)")


# ========================
# 11. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evaluation Report
# ========================
evaluation_report = {
    'training_config': training_config,
    'test_results': {
        'original_model': float(original_score),
        'finetuned_model': float(finetuned_score),
        'improvement': float(improvement['spearman_correlation'])
    },
    'pairwise_metrics': {
        'threshold': pair_metrics['threshold'],
        'precision': pair_metrics['precision'],
        'recall': pair_metrics['recall'],
        'f1': pair_metrics['f1'],
        'accuracy': pair_metrics['accuracy'],
        'spearman': pair_metrics['spearman'],
        'confusion_matrix': pair_metrics['confusion_matrix'],
        'plots': {
            'similarity_distribution': dist_path,
            'label_vs_similarity': scatter_path,
            'confusion_matrix': cm_path
        }
    },
    'model_info': {
        'base_model': 'BAAI/bge-base-en-v1.5',
        'finetuned_path': output_path,
        'embedding_path': embedding_path,
        'embedding_dim': 768,
        'num_movies': len(movies_df)
    },
    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}

report_path = f'{output_path}/evaluation_report.json'
with open(report_path, 'w', encoding='utf-8') as f:
    json.dump(evaluation_report, f, indent=2, ensure_ascii=False)

print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evaluation report ‡∏ó‡∏µ‡πà: {report_path}")


# ========================
# 12. ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
# ========================
print("\n" + "=" * 80)
print("‚úÖ ‡∏Å‡∏≤‡∏£ Fine-tuning ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
print("=" * 80)

print("\nüìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô:")
print(f"  1. Model: {output_path}/")
print(f"  2. Embeddings: {embedding_path}")
print(f"  3. Config: {output_path}/training_config.json")
print(f"  4. Report: {report_path}")
print(f"  5. Plots: {metrics_dir}/")
print(f"     - similarity_distribution.png")
print(f"     - label_vs_similarity.png")
print(f"     - confusion_matrix.png")

print("\nüîß ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Fine-tuned Model ‡πÉ‡∏ô app.py:")
print("  ‡πÅ‡∏Å‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î 317-318:")
print("  ")
print("  # ‡πÄ‡∏î‡∏¥‡∏°:")
print("  model = SentenceTransformer('BAAI/bge-base-en-v1.5', device='cpu')")
print("  movie_vectors = np.load('data/movie_embeddings.npy')")
print("  ")
print("  # ‡πÉ‡∏´‡∏°‡πà:")
print("  model = SentenceTransformer('models/movie-mpnet-finetuned', device='cpu')")
print("  movie_vectors = np.load('data/movie_embeddings_finetuned.npy')")

print("\nüöÄ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏£‡∏±‡∏ô Flask app ‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥:")
print("  python app.py")

print("\n" + "=" * 80)
