"""
‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ Deep Learning
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤ Neural Network & Deep Learning

‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå AI ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á:
‚ú® ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏±‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á 16,904 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
üß† Dual Transformer Architecture:
   ‚Ä¢ Bi-Encoder: BAAI/bge-base-en-v1.5 (SOTA Embedding)
   ‚Ä¢ Cross-Encoder: ms-marco-MiniLM-L-12-v2 (Deep Re-ranker)
üéØ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ AI 6 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:
   Intent Analysis ‚Üí Semantic Search ‚Üí Hybrid Scoring ‚Üí 
   Cross-Encoder Re-ranking ‚Üí Diversity Optimization ‚Üí Results
üí° Intent-Aware Weighting:
   ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à (recent/classic/quality/popular/niche)
üåè ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: 80+ ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î + 15+ ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥
üìä Metadata Fusion: Token-level matching (keywords, cast, directors)
üìÖ Year Intelligence: ‡∏ó‡∏®‡∏ß‡∏£‡∏£‡∏©, ‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏µ, ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤
üé¨ Genre Hints: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡∏±‡∏á
‚ö° ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ú‡∏™‡∏°‡πÅ‡∏ö‡∏ö‡πÑ‡∏î‡∏ô‡∏≤‡∏°‡∏¥‡∏Å:
   Semantic (35-65%) + Metadata (10-35%) + Quality (5-30%) + 
   Popularity (5-25%) + Recency (5-30%) + Year/Genre Bonuses
üîç Cross-Encoder Re-ranking: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 30 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
üé® Dynamic Diversity: ‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ (0.05-0.20) ‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
"""

from flask import Flask, render_template, request, jsonify
import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, CrossEncoder
from sklearn.metrics.pairwise import cosine_similarity
import re
from datetime import datetime
import os

app = Flask(__name__)

from tmdb_client import TMDBClient
# ‡∏≠‡πà‡∏≤‡∏ô API Key ‡∏à‡∏≤‡∏Å Environment Variable ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
# export TMDB_API_KEY='your_key_here'
tmdb_client = TMDBClient()

# --- ‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢-‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ---
THAI_TO_ENGLISH = {
    '‡πÅ‡∏≠‡∏Ñ‡∏ä‡∏±‡πà‡∏ô': 'action',
    '‡∏ö‡∏π‡πä': 'action thriller fighting',
    '‡∏ï‡∏•‡∏Å': 'comedy funny',
    '‡∏ï‡∏•‡∏Å‡∏Ç‡∏≥‡∏Ç‡∏±‡∏ô': 'comedy hilarious',
    '‡∏ú‡∏µ': 'horror',
    '‡∏™‡∏¢‡∏≠‡∏á‡∏Ç‡∏ß‡∏±‡∏ç': 'horror scary',
    '‡πÇ‡∏£‡πÅ‡∏°‡∏ô‡∏ï‡∏¥‡∏Å': 'romance',
    '‡∏£‡∏±‡∏Å‡πÇ‡∏£‡πÅ‡∏°‡∏ô‡∏ï‡∏¥‡∏Å': 'romantic love',
    '‡∏î‡∏£‡∏≤‡∏°‡πà‡∏≤': 'drama',
    '‡∏î‡∏£‡∏≤‡∏°‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô': 'intense drama',
    '‡πÅ‡∏ü‡∏ô‡∏ï‡∏≤‡∏ã‡∏µ': 'fantasy',
    '‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå': 'science fiction',
    '‡πÑ‡∏ã‡πÑ‡∏ü': 'sci-fi science fiction',
    '‡∏£‡∏∞‡∏ó‡∏∂‡∏Å‡∏Ç‡∏ß‡∏±‡∏ç': 'thriller',
    '‡∏•‡∏∂‡∏Å‡∏•‡∏±‡∏ö': 'mystery',
    '‡∏õ‡∏£‡∏¥‡∏®‡∏ô‡∏≤': 'mystery puzzle',
    '‡∏ú‡∏à‡∏ç‡∏†‡∏±‡∏¢': 'adventure',
    '‡∏≠‡∏≤‡∏ä‡∏ç‡∏≤‡∏Å‡∏£‡∏£‡∏°': 'crime',
    '‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°': 'war',
    '‡∏™‡∏≤‡∏£‡∏Ñ‡∏î‡∏µ': 'documentary',
    '‡πÅ‡∏≠‡∏ô‡∏ô‡∏¥‡πÄ‡∏°‡∏ä‡∏±‡πà‡∏ô': 'animation',
    '‡∏Å‡∏≤‡∏£‡πå‡∏ï‡∏π‡∏ô': 'animation',
    '‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß': 'family',
    '‡∏ã‡∏π‡πÄ‡∏õ‡∏≠‡∏£‡πå‡∏Æ‡∏µ‡πÇ‡∏£‡πà': 'superhero marvel dc',
    '‡∏Æ‡∏µ‡πÇ‡∏£‡πà': 'hero superhero',
    '‡∏ã‡∏≠‡∏°‡∏ö‡∏µ‡πâ': 'zombie',
    '‡πÅ‡∏ß‡∏°‡πÑ‡∏û‡∏£‡πå': 'vampire',
    '‡∏°‡∏±‡∏á‡∏Å‡∏£': 'dragon',
    '‡πÄ‡∏ß‡∏ó‡∏°‡∏ô‡∏ï‡∏£‡πå': 'magic wizard',
    '‡∏≠‡∏ß‡∏Å‡∏≤‡∏®': 'space',
    '‡πÄ‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏ô': 'alien',
    '‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå': 'robot',
    '‡πÑ‡∏î‡πÇ‡∏ô‡πÄ‡∏™‡∏≤‡∏£‡πå': 'dinosaur',
    '‡πÇ‡∏à‡∏£‡∏™‡∏•‡∏±‡∏î': 'pirate',
    '‡∏ô‡∏¥‡∏ô‡∏à‡∏≤': 'ninja',
    '‡∏ã‡∏≤‡∏°‡∏π‡πÑ‡∏£': 'samurai',
    '‡∏°‡∏≤‡πÄ‡∏ü‡∏µ‡∏¢': 'mafia gangster',
    '‡πÄ‡∏®‡∏£‡πâ‡∏≤': 'sad emotional',
    '‡∏™‡∏ô‡∏∏‡∏Å': 'fun entertaining',
    '‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô': 'exciting thrilling',
    '‡∏ô‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ß': 'scary frightening',
    '‡∏™‡∏∞‡πÄ‡∏ó‡∏∑‡∏≠‡∏ô‡πÉ‡∏à': 'touching emotional',
    '‡∏Æ‡∏≤': 'funny hilarious',
    '‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ': 'korean',
    '‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô': 'japanese',
    '‡πÑ‡∏ó‡∏¢': 'thai',
    '‡∏à‡∏µ‡∏ô': 'chinese',
    '‡∏ù‡∏£‡∏±‡πà‡∏á': 'western',
    '‡∏≠‡∏¥‡∏ô‡πÄ‡∏î‡∏µ‡∏¢': 'indian bollywood',
    '‡∏Æ‡∏≠‡∏•‡∏•‡∏µ‡∏ß‡∏π‡∏î': 'hollywood',
    '‡∏ö‡∏≠‡∏•‡∏•‡∏µ‡∏ß‡∏π‡∏î': 'bollywood',
}

QUERY_EXPANSION = {
    'mind-bending': 'psychological complex inception interstellar matrix reality thought-provoking cerebral',
    'mind bending': 'psychological complex inception interstellar matrix reality thought-provoking cerebral',
    'emotional': 'touching heartwarming tearjerker moving powerful drama feelings',
    'intense': 'gripping suspenseful edge-of-seat thrilling powerful',
    'dark': 'noir gritty moody atmospheric bleak',
    'uplifting': 'inspiring feel-good heartwarming positive motivational',
    'epic': 'grand spectacular massive ambitious large-scale',
    'slow burn': 'contemplative meditative paced atmospheric',
    'fast-paced': 'action-packed exciting dynamic energetic',
    'visually stunning': 'beautiful cinematography visual-effects gorgeous',
    'indie': 'independent art-house alternative',
    'twist': 'plot-twist surprise unexpected revelation',
    'character-driven': 'character-study psychological drama performance',
    'based on true': 'true-story biographical real-events documentary',
}

GENRE_SYNONYMS = {
    'action': 'action thriller explosive fighting combat battle',
    'comedy': 'comedy funny hilarious humor laugh',
    'horror': 'horror scary frightening terror',
    'romance': 'romance romantic love relationship',
    'drama': 'drama emotional intense character-driven',
    'scifi': 'science fiction sci-fi futuristic technology',
    'sci-fi': 'science fiction futuristic technology space',
    'fantasy': 'fantasy magical mystical enchanted',
    'thriller': 'thriller suspense tension mystery',
    'adventure': 'adventure journey quest exploration',
    'crime': 'crime detective investigation police',
    'superhero': 'superhero marvel dc comic-book powers hero',
}

GENRE_HINT_KEYWORDS = {
    'science fiction': 'Science Fiction',
    'sci-fi': 'Science Fiction',
    'scifi': 'Science Fiction',
    '‡πÑ‡∏ã‡πÑ‡∏ü': 'Science Fiction',
    'mind-bending sci-fi': 'Science Fiction',
    'mind bending sci-fi': 'Science Fiction',
    'thriller': 'Thriller',
    'psychological': 'Thriller',
    'drama': 'Drama',
    'animation': 'Animation',
    'animated': 'Animation',
    'documentary': 'Documentary',
    'romance': 'Romance',
    'action': 'Action',
    'adventure': 'Adventure',
    'comedy': 'Comedy',
    'horror': 'Horror',
}

BASE_WEIGHT_PROFILE = {
    'semantic': 0.50,
    'metadata': 0.20,
    'quality': 0.15,
    'popularity': 0.10,
    'recency': 0.05,
}

WEIGHT_LIMITS = {
    'semantic': (0.35, 0.65),
    'metadata': (0.10, 0.35),
    'quality': (0.05, 0.30),
    'popularity': (0.05, 0.25),
    'recency': (0.05, 0.30),
}

RECENT_KEYWORDS = ['new', 'latest', 'recent', 'modern', 'fresh', '‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô', '‡πÉ‡∏´‡∏°‡πà', '‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î']
CLASSIC_KEYWORDS = ['classic', 'retro', 'vintage', 'old school', 'nostalgic', '‡∏¢‡∏∏‡∏Ñ‡πÄ‡∏Å‡πà‡∏≤', '‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏¥‡∏Ñ', '‡∏ï‡∏≥‡∏ô‡∏≤‡∏ô']
QUALITY_KEYWORDS = ['award', 'oscar', 'acclaimed', 'masterpiece', 'critically', '‡∏Å‡∏≤‡∏£‡∏±‡∏ô‡∏ï‡∏µ', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•']
POPULAR_KEYWORDS = ['popular', 'hit', 'blockbuster', 'top grossing', 'box office', '‡∏Æ‡∏¥‡∏ï', '‡∏î‡∏±‡∏á']
NICHE_KEYWORDS = ['underrated', 'hidden gem', 'cult', 'obscure', 'indie', '‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏î‡∏±‡∏á', '‡∏•‡∏±‡∏ö']
METADATA_KEYWORDS = ['starring', 'directed by', 'actor', 'cast', 'director', '‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö', '‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á']
DIVERSITY_KEYWORDS = ['variety', 'surprise', 'mix', '‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢', '‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á']
FOCUSED_KEYWORDS = ['similar', 'exact', '‡πÄ‡∏â‡∏û‡∏≤‡∏∞', '‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô']

DECADE_KEYWORDS = {
    '60s': (1960, 1969),
    "60's": (1960, 1969),
    '‡∏¢‡∏∏‡∏Ñ 60': (1960, 1969),
    '70s': (1970, 1979),
    "70's": (1970, 1979),
    '‡∏¢‡∏∏‡∏Ñ 70': (1970, 1979),
    '80s': (1980, 1989),
    "80's": (1980, 1989),
    '‡∏¢‡∏∏‡∏Ñ 80': (1980, 1989),
    '90s': (1990, 1999),
    "90's": (1990, 1999),
    '‡∏¢‡∏∏‡∏Ñ 90': (1990, 1999),
    '2000s': (2000, 2009),
    '2010s': (2010, 2019),
    "2010's": (2010, 2019),
}

TOKEN_PATTERN = re.compile(r"[a-zA-Z0-9‡∏Å-‡πô']+")


def clamp(value, min_value, max_value):
    return max(min_value, min(max_value, value))


def tokenize_text(text):
    if not text:
        return set()
    return {match.group(0).lower() for match in TOKEN_PATTERN.finditer(str(text))}


def build_weight_profile(adjustments):
    weights = BASE_WEIGHT_PROFILE.copy()
    for key, delta in adjustments.items():
        if key in weights:
            low, high = WEIGHT_LIMITS.get(key, (0, 1))
            weights[key] = clamp(weights[key] + delta, low, high)
    total = sum(weights.values())
    for key in weights:
        weights[key] /= total
    return weights


def extract_genre_hints(query_lower):
    hints = set()
    for keyword, canonical in GENRE_HINT_KEYWORDS.items():
        if keyword in query_lower:
            hints.add(canonical)
    return hints


def extract_year_constraints(query_lower):
    constraints = {'min_year': None, 'max_year': None}
    between_match = re.search(r"(?:between|‡∏ä‡πà‡∏ß‡∏á)\s+(?:‡∏õ‡∏µ\s*)?((?:19|20)\d{2})\s+(?:and|‡∏ñ‡∏∂‡∏á)\s+(?:‡∏õ‡∏µ\s*)?((?:19|20)\d{2})", query_lower)
    if between_match:
        constraints['min_year'] = int(between_match.group(1))
        constraints['max_year'] = int(between_match.group(2))
        return constraints
    after_match = re.search(r"(?:after|since|‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà|‡∏´‡∏•‡∏±‡∏á)\s+(?:‡∏õ‡∏µ\s*)?((?:19|20)\d{2})", query_lower)
    if after_match:
        constraints['min_year'] = int(after_match.group(1))
    before_match = re.search(r"(?:before|‡∏Å‡πà‡∏≠‡∏ô)\s+(?:‡∏õ‡∏µ\s*)?((?:19|20)\d{2})", query_lower)
    if before_match:
        constraints['max_year'] = int(before_match.group(1))
    decade_hit = next(((start, end) for phrase, (start, end) in DECADE_KEYWORDS.items() if phrase in query_lower), None)
    if decade_hit:
        constraints['min_year'], constraints['max_year'] = decade_hit
    single_years = [int(year) for year in re.findall(r"((?:19|20)\d{2})", query_lower)]
    if single_years and not between_match and not (after_match or before_match):
        year = single_years[0]
        constraints['min_year'] = year
        constraints['max_year'] = year
    return constraints


def calculate_year_alignment_bonus(release_year, constraints):
    if not constraints or (constraints.get('min_year') is None and constraints.get('max_year') is None):
        return 0.0
    if pd.isna(release_year):
        return 0.0
    year = int(release_year)
    bonus = 0.0
    min_year = constraints.get('min_year')
    max_year = constraints.get('max_year')
    if min_year:
        bonus += 0.04 if year >= min_year else -0.05
    if max_year:
        bonus += 0.04 if year <= max_year else -0.05
    return clamp(bonus, -0.08, 0.08)


def analyze_query_intent(query):
    query_lower = query.lower()
    adjustments = {'metadata': 0.0, 'quality': 0.0, 'popularity': 0.0, 'recency': 0.0}
    diversity_penalty = 0.1
    constraints = extract_year_constraints(query_lower)
    genre_hints = extract_genre_hints(query_lower)
    if any(keyword in query_lower for keyword in RECENT_KEYWORDS):
        adjustments['recency'] += 0.06
        if constraints.get('min_year') is None:
            constraints['min_year'] = max(1900, current_year - 10)
    if any(keyword in query_lower for keyword in CLASSIC_KEYWORDS):
        adjustments['recency'] -= 0.05
        if constraints.get('max_year') is None:
            constraints['max_year'] = min(constraints.get('max_year') or current_year, 2005)
    if any(keyword in query_lower for keyword in QUALITY_KEYWORDS):
        adjustments['quality'] += 0.04
    if any(keyword in query_lower for keyword in POPULAR_KEYWORDS):
        adjustments['popularity'] += 0.04
    if any(keyword in query_lower for keyword in NICHE_KEYWORDS):
        adjustments['popularity'] -= 0.04
    if any(keyword in query_lower for keyword in METADATA_KEYWORDS):
        adjustments['metadata'] += 0.05
    if any(keyword in query_lower for keyword in DIVERSITY_KEYWORDS):
        diversity_penalty = 0.05
    if any(keyword in query_lower for keyword in FOCUSED_KEYWORDS):
        diversity_penalty = 0.15
    weights = build_weight_profile(adjustments)
    return {
        'weights': weights,
        'year_constraints': constraints,
        'diversity_penalty': clamp(diversity_penalty, 0.05, 0.2),
        'genre_hints': genre_hints,
    }


def get_best_device():
    """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å device ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö: CUDA GPU > MPS (Apple Silicon) > CPU"""
    if torch.cuda.is_available():
        print(f"‚úÖ ‡∏û‡∏ö CUDA GPU: {torch.cuda.get_device_name(0)}")
        return 'cuda'
    elif torch.backends.mps.is_available():
        print("‚úÖ ‡∏û‡∏ö Apple Silicon MPS - ‡πÉ‡∏ä‡πâ GPU acceleration")
        return 'mps'
    else:
        print("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU - ‡πÉ‡∏ä‡πâ CPU (‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤)")
        return 'cpu'

print("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î AI model ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
device = get_best_device()

# ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà fine-tune ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≠‡∏¢‡πÉ‡∏ä‡πâ base model
finetuned_path = 'data/finetuned_model'
try:
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ config.json (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)
    import json
    config_path = os.path.join(finetuned_path, 'config.json')
    if os.path.exists(config_path):
        print("üéØ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà fine-tune ‡πÅ‡∏•‡πâ‡∏ß (‡∏â‡∏•‡∏≤‡∏î‡∏Å‡∏ß‡πà‡∏≤!)")
        model = SentenceTransformer(finetuned_path, device=device)
        model_name = 'movie-finetuned-bge'
    else:
        raise FileNotFoundError("Fine-tuned model ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à")
except (FileNotFoundError, ValueError, OSError) as e:
    print(f"üìö ‡πÇ‡∏´‡∏•‡∏î SOTA base model (fine-tuned model ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°: {e})")
    # ‡πÉ‡∏ä‡πâ BGE-base-en-v1.5 ‡πÅ‡∏ó‡∏ô all-mpnet-base-v2
    model_name = 'BAAI/bge-base-en-v1.5'
    print(f"üöÄ Model: {model_name}")
    model = SentenceTransformer(model_name, device=device)

print("üß† Loading Cross-Encoder re-ranker (Deep Layer)...")
try:
    # ‡πÉ‡∏ä‡πâ L-12 (‡∏â‡∏•‡∏≤‡∏î‡∏Å‡∏ß‡πà‡∏≤ L-6)
    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', device=device)
    use_reranker = True
    print(f"‚úÖ Cross-Encoder (L-12) loaded successfully on {device.upper()}")
except Exception as e:
    print(f"‚ö†Ô∏è  Cross-Encoder not available: {e}")
    print("üí° Run: pip install sentence-transformers --upgrade")
    use_reranker = False

movies_df = pd.read_pickle('data/movies.pkl')

# ‡πÇ‡∏´‡∏•‡∏î embeddings ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
if os.path.exists('data/movie_embeddings_finetuned.npy'):
    print("üìä ‡πÇ‡∏´‡∏•‡∏î fine-tuned embeddings (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤!)")
    movie_vectors = np.load('data/movie_embeddings_finetuned.npy')
else:
    print("üìä ‡πÇ‡∏´‡∏•‡∏î base embeddings")
    movie_vectors = np.load('data/movie_embeddings.npy')

movies_df['popularity_score'] = (movies_df['popularity'] - movies_df['popularity'].min()) / \
                                 (movies_df['popularity'].max() - movies_df['popularity'].min())
current_year = datetime.now().year
movies_df['recency_score'] = movies_df['release_year'].apply(
    lambda year: max(0, 1 - (current_year - year) / 100)
)
movies_df['quality_score'] = movies_df['vote_average'] / 10.0
print(f"‚úÖ Ready! {len(movies_df):,} movies with 768-dim embeddings")


def translate_thai_keywords(text):
    text_lower = text.lower()
    translated = text
    for thai, english in THAI_TO_ENGLISH.items():
        if thai in text_lower:
            translated = translated.replace(thai, english)
            print(f"üåê Translated: '{thai}' ‚Üí '{english}'")
    return translated


from deep_translator import GoogleTranslator

def enhance_query(query):
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Dynamic Translation)
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏Å-‡πô) ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    if re.search(r'[‡∏Å-‡πô]', query):
        try:
            print(f"üáπüá≠ ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: '{query}'")
            translated = GoogleTranslator(source='auto', target='en').translate(query)
            print(f"üá¨üáß ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©: '{translated}'")
            query_en = translated
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
            query_en = translate_thai_keywords(query) # Fallback
    else:
        query_en = translate_thai_keywords(query)

    query_lower = query_en.lower()
    expanded_parts = []
    for phrase, expansion in QUERY_EXPANSION.items():
        if phrase in query_lower:
            expanded_parts.append(expansion)
            print(f"üí° ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥ '{phrase}' ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏î‡πâ‡∏≤‡∏ô")
    for genre, synonyms in GENRE_SYNONYMS.items():
        if genre in query_lower:
            expanded_parts.append(synonyms)
            print(f"üé¨ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏û‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ô‡∏ß '{genre}'")
    if expanded_parts:
        query_en = f"{query_en} {' '.join(expanded_parts)}"
    print(f"üîç ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß: '{query_en[:100]}...'" if len(query_en) > 100 else f"üîç ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß: '{query_en}'")
    return query_en


def calculate_metadata_score(query_text, movie_row, query_tokens=None):
    query_lower = query_text.lower()
    lookup_tokens = query_tokens or tokenize_text(query_text)
    score = 0.0
    keywords = movie_row['keywords'] if isinstance(movie_row['keywords'], list) else []
    if keywords:
        matches = 0
        for kw in keywords[:20]:
            kw_tokens = tokenize_text(kw)
            if kw_tokens & lookup_tokens or kw.lower() in query_lower:
                matches += 1
        score += min(matches, 4) * 0.05
    director = movie_row['director'] if isinstance(movie_row['director'], str) else ''
    if director:
        director_tokens = tokenize_text(director)
        if director.lower() in query_lower or director_tokens & lookup_tokens:
            score += 0.10
    cast_list = movie_row['cast'] if isinstance(movie_row['cast'], list) else []
    if cast_list:
        cast_matches = 0
        for actor in cast_list[:5]:
            actor_tokens = tokenize_text(actor)
            actor_key = actor.lower()
            if actor_key in query_lower or actor_tokens & lookup_tokens:
                cast_matches += 1
        score += cast_matches * 0.03
    return min(score, 0.35)


def calculate_diversity_penalty(selected_indices, candidate_idx, movies_df, penalty=0.1):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì penalty ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á genre
    ‡πÑ‡∏°‡πà penalize ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ô‡∏•‡∏∞‡∏õ‡∏µ (remake/sequel)
    """
    if len(selected_indices) == 0:
        return 0
    
    candidate_movie = movies_df.iloc[candidate_idx]
    candidate_title = candidate_movie['title']
    candidate_year = candidate_movie['release_year']
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏ô‡∏±‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÉ‡∏ô selected ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    for idx in selected_indices:
        selected_movie = movies_df.iloc[idx]
        if selected_movie['title'] == candidate_title and selected_movie['release_year'] != candidate_year:
            # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÅ‡∏ï‡πà‡∏Ñ‡∏ô‡∏•‡∏∞‡∏õ‡∏µ = ‡πÑ‡∏°‡πà penalize (‡πÄ‡∏õ‡πá‡∏ô remake/reboot)
            return 0
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì genre overlap ‡∏õ‡∏Å‡∏ï‡∏¥
    selected_genres = set()
    for idx in selected_indices:
        selected_genres.update(movies_df.iloc[idx]['genres'])
    candidate_genres = set(candidate_movie['genres'])
    overlap = len(selected_genres.intersection(candidate_genres))
    return overlap * penalty


def get_recommendations_advanced(query, top_n=10, diversity=True):
    enhanced_query = enhance_query(query)
    intent_context = analyze_query_intent(query)
    weights = intent_context['weights']
    year_constraints = intent_context['year_constraints']
    diversity_penalty = intent_context['diversity_penalty']
    genre_hints = intent_context['genre_hints']
    metadata_reference_text = f"{query} {enhanced_query}"
    metadata_query_tokens = tokenize_text(metadata_reference_text)

    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢...")
    query_vector = model.encode([enhanced_query], normalize_embeddings=True)
    semantic_scores = cosine_similarity(query_vector, movie_vectors).flatten()
    top_100_indices = np.argsort(semantic_scores)[-100:][::-1]

    print("üßÆ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ú‡∏™‡∏°‡∏î‡πâ‡∏ß‡∏¢ metadata fusion...")
    hybrid_scores = []
    for idx in top_100_indices:
        movie_row = movies_df.iloc[idx]
        semantic = semantic_scores[idx]
        quality = movie_row['quality_score']
        popularity = movie_row['popularity_score']
        recency = movie_row['recency_score']
        metadata = calculate_metadata_score(metadata_reference_text, movie_row, metadata_query_tokens)
        year_bonus = calculate_year_alignment_bonus(movie_row['release_year'], year_constraints)
        genre_bonus = 0.0
        if genre_hints:
            genre_matches = sum(1 for hint in genre_hints if hint in movie_row['genres'])
            if genre_matches:
                genre_bonus += clamp(genre_matches * 0.04, 0.0, 0.12)
            else:
                genre_bonus -= 0.04
        hybrid = (semantic * weights['semantic'] +
              metadata * weights['metadata'] +
              quality * weights['quality'] +
              popularity * weights['popularity'] +
              recency * weights['recency'] +
              year_bonus +
              genre_bonus)
        hybrid_scores.append((idx, hybrid, semantic))
    hybrid_scores.sort(key=lambda x: x[1], reverse=True)

    if use_reranker and len(hybrid_scores) >= 30:
        print("üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: Cross-Encoder ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 30 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏£‡∏Å‡πÉ‡∏´‡∏°‡πà...")
        top_30 = hybrid_scores[:30]
        pairs = []
        for idx, _, _ in top_30:
            movie = movies_df.iloc[idx]
            doc_text = f"{movie['title']}. {movie['overview']}. ‡πÅ‡∏ô‡∏ß: {', '.join(movie['genres'][:3])}"
            pairs.append([query, doc_text])
        try:
            ce_scores = reranker.predict(pairs)
            reranked = []
            for i, (idx, hybrid, semantic) in enumerate(top_30):
                final_score = ce_scores[i] * 0.7 + hybrid * 0.3
                reranked.append((idx, final_score, semantic))
            reranked.sort(key=lambda x: x[1], reverse=True)
            hybrid_scores = reranked + hybrid_scores[30:]
            print("‚úÖ Cross-Encoder re-ranking ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
        except Exception as e:
            print(f"‚ö†Ô∏è  Cross-Encoder ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}, ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô hybrid")

    if diversity:
        print("üé® ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢...")
        final_indices = []
        final_scores = []
        final_hybrid_scores = []
        seen_movies = set()
        for idx, hybrid_score, semantic_score in hybrid_scores:
            if len(final_indices) >= top_n:
                break
            movie_key = (movies_df.iloc[idx]['title'], movies_df.iloc[idx]['release_year'])
            if movie_key in seen_movies:
                continue
            penalty = calculate_diversity_penalty(final_indices, idx, movies_df, penalty=diversity_penalty)
            adjusted_score = hybrid_score - penalty
            final_indices.append(idx)
            final_scores.append(semantic_score)
            final_hybrid_scores.append(adjusted_score)
            seen_movies.add(movie_key)
        recommendations = movies_df.iloc[final_indices].copy()
        match_scores = np.array(final_scores)
        hybrid_match_scores = np.array(final_hybrid_scores)
    else:
        final_indices = []
        final_scores = []
        final_hybrid_scores = []
        seen_movies = set()
        for idx, hybrid_score, semantic_score in hybrid_scores:
            if len(final_indices) >= top_n:
                break
            movie_key = (movies_df.iloc[idx]['title'], movies_df.iloc[idx]['release_year'])
            if movie_key in seen_movies:
                continue
            final_indices.append(idx)
            final_scores.append(semantic_score)
            final_hybrid_scores.append(hybrid_score)
            seen_movies.add(movie_key)
        recommendations = movies_df.iloc[final_indices].copy()
        match_scores = np.array(final_scores)
        hybrid_match_scores = np.array(final_hybrid_scores)

    return recommendations, match_scores, hybrid_match_scores


@app.route('/')
def home():
    return render_template('index.html')


@app.route('/search', methods=['POST'])
def search():
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        if not query:
            return jsonify({'error': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤'}), 400
        recommendations, semantic_scores, hybrid_scores = get_recommendations_advanced(query, top_n=10)
        results = []
        for idx, (_, row) in enumerate(recommendations.iterrows()):
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì score breakdown ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN
            semantic_val = semantic_scores[idx] if not np.isnan(semantic_scores[idx]) else 0.0
            hybrid_val = hybrid_scores[idx] if not np.isnan(hybrid_scores[idx]) else 0.0
            
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á hybrid ‡∏Å‡∏±‡∏ö semantic ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö
            # hybrid ‡∏≠‡∏≤‡∏à‡∏ï‡∏¥‡∏î‡∏•‡∏ö‡∏à‡∏≤‡∏Å diversity penalty ‡∏´‡∏£‡∏∑‡∏≠ genre mismatch
            final_score = max(hybrid_val, semantic_val * 0.8)  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏î‡πâ 80% ‡∏Ç‡∏≠‡∏á semantic
            
            # Normalize ‡πÄ‡∏õ‡πá‡∏ô 0-100%
            # Semantic score ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-1 ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (cosine similarity)
            # Hybrid score ‡∏õ‡∏Å‡∏ï‡∏¥‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà 0-1.2 ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏ï‡∏¥‡∏î‡∏•‡∏ö‡∏à‡∏≤‡∏Å penalty
            match_pct = float(min(max(final_score, 0.0) * 100, 100.0))
            semantic_pct = float(min(semantic_val * 100, 100.0))
            
            quality_pct = float(row['vote_average'] * 10) if pd.notna(row['vote_average']) else 0.0
            popularity_pct = float(row['popularity_score'] * 100) if pd.notna(row['popularity_score']) else 0.0
            recency_pct = float(row['recency_score'] * 100) if pd.notna(row['recency_score']) else 0.0
            
            results.append({
                'title': row['title'],
                'year': int(row['release_year']) if pd.notna(row['release_year']) and row['release_year'] > 0 else 'N/A',
                'rating': float(row['vote_average']) if pd.notna(row['vote_average']) else 0.0,
                'match': match_pct,
                'score_breakdown': {
                    'semantic': round(semantic_pct, 1),
                    'quality': round(quality_pct, 1),
                    'popularity': round(popularity_pct, 1),
                    'recency': round(recency_pct, 1),
                },
                'overview': row['overview'] if pd.notna(row['overview']) else '‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡πà‡∏≠',
                'genres': row['genres'][:3] if isinstance(row['genres'], list) else [],
                'director': row['director'] if pd.notna(row['director']) and row['director'] else '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö',
                'cast': row['cast'][:3] if isinstance(row['cast'], list) and len(row['cast']) > 0 else [],
                'keywords': row['keywords'][:5] if isinstance(row['keywords'], list) and len(row['keywords']) > 0 else [],
            })
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° match % ‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
        results.sort(key=lambda x: x['match'], reverse=True)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡πâ‡∏ß
        for idx, result in enumerate(results):
            result['rank'] = idx + 1
            
        # --- TMDB Enrichment (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Top 10) ---
        # ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö Parallel ‡∏´‡∏£‡∏∑‡∏≠ Batch ‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏≥ Loop ‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
        # (‡πÉ‡∏ô Production ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ ThreadPoolExecutor)
        if tmdb_client.api_key:
            print("üé® Fetching TMDB images...")
            for res in results:
                try:
                    enrichment = tmdb_client.enrich_movie_data(res['title'], res['year'])
                    if enrichment:
                        res.update(enrichment)
                except Exception as e:
                    print(f"‚ö†Ô∏è TMDB Error for {res['title']}: {e}")
        
        return jsonify({'query': query, 'count': len(results), 'results': results})
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/stats')
def stats():
    return jsonify({
        'total_movies': len(movies_df),
        'year_range': {'min': int(movies_df['release_year'].min()), 'max': int(movies_df['release_year'].max())},
        'average_rating': float(movies_df['vote_average'].mean()),
        'model': model_name,
        'device': device,
        'embedding_dims': 768,
        'total_votes': int(movies_df['vote_count'].sum())
    })


if __name__ == '__main__':
    print("\n" + "=" * 80)
    print("üéì ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á")
    print("   ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤ Neural Network & Deep Learning")
    print("=" * 80)
    print(f"\nüìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(movies_df):,} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
    print(f"üß† Bi-Encoder: {model_name} (SOTA)")
    print(f"‚ö° Device: {device.upper()}")
    if use_reranker:
        print("üéØ Cross-Encoder: ms-marco-MiniLM-L-12-v2 (Deep re-ranking)")
    print("‚ú® ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ AI: 4 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (Semantic ‚Üí Hybrid ‚Üí Cross-Encoder ‚Üí Diversity)")
    print("üí° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏î‡πâ‡∏≤‡∏ô: ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå/‡∏™‡πÑ‡∏ï‡∏•‡πå/‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
    print("üìä Metadata Fusion: ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà Keywords + ‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á + ‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö")
    print("üåè ‡∏†‡∏≤‡∏©‡∏≤: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ó‡∏¢ + ‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©")
    print("\nüåê Access at: http://localhost:8000")
    print("=" * 80 + "\n")
    # ‡πÉ‡∏ä‡πâ port 8000 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà gunicorn ‡∏à‡∏∞‡πÉ‡∏ä‡πâ
    app.run(debug=True, host='0.0.0.0', port=8000)
