"""
Fine-tune Sentence Transformer ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
‡πÉ‡∏ä‡πâ Multiple Negatives Ranking Loss ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á

GPU ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å (10-20 ‡πÄ‡∏ó‡πà‡∏≤)
Mac M2 ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 1-1.5 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (GPU) ‡∏´‡∏£‡∏∑‡∏≠ 3-4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (CPU)
"""

import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
import random
from tqdm import tqdm
import os

# ‡∏ï‡∏±‡πâ‡∏á random seed
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)


def get_best_device():
    """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å device ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î - GPU ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠"""
    if torch.cuda.is_available():
        device = 'cuda'
        print(f"‚úÖ ‡πÉ‡∏ä‡πâ CUDA GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    elif torch.backends.mps.is_available():
        device = 'mps'
        print("‚úÖ ‡πÉ‡∏ä‡πâ Apple Silicon GPU (MPS)")
        print("   ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏∞‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤ CPU (‡πÉ‡∏ä‡πâ batch_size=4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory)")
    else:
        device = 'cpu'
        print("‚ö†Ô∏è  ‡πÉ‡∏ä‡πâ CPU - ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô")
        print("üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ GPU ‡∏´‡∏£‡∏∑‡∏≠ Apple Silicon")
    return device


def create_training_examples(movies_df, num_examples=5000):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏±‡∏á‡∏à‡∏£‡∏¥‡∏á
    
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå:
    - Positive pairs: ‡∏´‡∏ô‡∏±‡∏á‡πÅ‡∏ô‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô, ‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô, keywords ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
    - ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (query, positive) pairs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Multiple Negatives Ranking Loss
    """
    print("\nüìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏±‡∏á‡∏à‡∏£‡∏¥‡∏á...")
    
    examples = []
    
    # 1. Genre-based pairs (‡∏´‡∏ô‡∏±‡∏á‡πÅ‡∏ô‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏£‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô)
    print("   üé¨ ‡∏™‡∏£‡πâ‡∏≤‡∏á genre-based pairs...")
    for _ in tqdm(range(num_examples // 3), desc="Genre"):
        # ‡∏™‡∏∏‡πà‡∏°‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡∏±‡∏á
        all_genres = set()
        for genres in movies_df['genres']:
            all_genres.update(genres)
        
        if len(all_genres) == 0:
            continue
            
        genre = random.choice(list(all_genres))
        
        # ‡∏´‡∏≤‡∏´‡∏ô‡∏±‡∏á‡πÉ‡∏ô‡πÅ‡∏ô‡∏ß‡∏ô‡∏µ‡πâ
        genre_movies = movies_df[movies_df['genres'].apply(lambda x: genre in x)]
        if len(genre_movies) >= 2:
            sample = genre_movies.sample(2)
            query = f"{genre} movie with great story"
            positive = f"{sample.iloc[0]['title']}. {sample.iloc[0]['overview']}"
            examples.append(InputExample(texts=[query, positive]))
    
    # 2. Director-based pairs (‡∏´‡∏ô‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏°‡∏µ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢)
    print("   üé• ‡∏™‡∏£‡πâ‡∏≤‡∏á director-based pairs...")
    for _ in tqdm(range(num_examples // 3), desc="Director"):
        directors = movies_df['director'].value_counts()
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏ô‡∏±‡∏á‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
        prolific_directors = directors[directors >= 2].index.tolist()
        if prolific_directors:
            director = random.choice(prolific_directors)
            director_movies = movies_df[movies_df['director'] == director]
            if len(director_movies) >= 2:
                sample = director_movies.sample(2)
                query = f"movie directed by {director}"
                positive = f"{sample.iloc[0]['title']}. {sample.iloc[0]['overview']}"
                examples.append(InputExample(texts=[query, positive]))
    
    # 3. Keyword-based pairs (keywords ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ = ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢)
    print("   üîë ‡∏™‡∏£‡πâ‡∏≤‡∏á keyword-based pairs...")
    for _ in tqdm(range(num_examples // 3), desc="Keyword"):
        movie1 = movies_df.sample(1).iloc[0]
        if isinstance(movie1['keywords'], list) and len(movie1['keywords']) > 0:
            # ‡∏´‡∏≤‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ keywords ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
            keyword = random.choice(movie1['keywords'])
            matching = movies_df[movies_df['keywords'].apply(
                lambda x: isinstance(x, list) and keyword in x
            )]
            if len(matching) >= 2:
                movie2 = matching[matching['title'] != movie1['title']]
                if len(movie2) > 0:
                    movie2 = movie2.sample(1).iloc[0]
                    query = f"movie about {keyword}"
                    positive = f"{movie2['title']}. {movie2['overview']}"
                    examples.append(InputExample(texts=[query, positive]))
    
    print(f"\n‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô {len(examples):,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
    return examples


def finetune_model(
    model_name='BAAI/bge-base-en-v1.5',
    epochs=3,
    batch_size=32,         # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 32 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 32GB RAM
    warmup_steps=500,
    output_path='data/finetuned_model'
):
    """
    Fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏±‡∏á‡∏à‡∏£‡∏¥‡∏á
    
    ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:
        epochs: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô (3-4 ‡∏£‡∏≠‡∏ö‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠)
        batch_size: ‡∏Ç‡∏ô‡∏≤‡∏î batch (16 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á, 32+ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU ‡πÅ‡∏£‡∏á)
        warmup_steps: Learning rate warmup (500-1000 ‡∏î‡∏µ)
    """
    print("=" * 80)
    print("üéì Fine-tuning Sentence Transformer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡∏±‡∏á")
    print("=" * 80)
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print("\nüì• ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏±‡∏á...")
    movies_df = pd.read_pickle('data/movies.pkl')
    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏ô‡∏±‡∏á {len(movies_df):,} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° device
    device = get_best_device()
    
    # ‡πÇ‡∏´‡∏•‡∏î base model
    print(f"\nüß† ‡πÇ‡∏´‡∏•‡∏î base model: {model_name}")
    model = SentenceTransformer(model_name, device=device)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    train_examples = create_training_examples(movies_df, num_examples=5000)
    
    # ‡∏õ‡∏£‡∏±‡∏ö batch_size ‡∏ï‡∏≤‡∏° device
    if device == 'cuda':
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        if gpu_memory < 8:
            batch_size = 4
            print(f"‚ö†Ô∏è  ‡∏õ‡∏£‡∏±‡∏ö batch_size ‡πÄ‡∏õ‡πá‡∏ô {batch_size} ‡πÄ‡∏û‡∏∑‡πà‡∏≠ VRAM")
    # MPS: ‡πÉ‡∏ä‡πâ batch size ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 32GB RAM ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ batch_size=32)
    # else:
    #     # CPU ‡πÅ‡∏•‡∏∞ MPS ‡πÉ‡∏ä‡πâ batch size ‡πÄ‡∏•‡πá‡∏Å
    #     batch_size = min(batch_size, 4)
    #     print(f"‚ö†Ô∏è  ‡∏õ‡∏£‡∏±‡∏ö batch_size ‡πÄ‡∏õ‡πá‡∏ô {batch_size} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {device.upper()}")
    #     if device == 'cpu':
    #         print("   (‡∏•‡∏î batch size ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô)")
    
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î loss function
    # Multiple Negatives Ranking Loss: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏´‡∏ô‡∏±‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô
    train_loss = losses.MultipleNegativesRankingLoss(model)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì warmup steps
    total_steps = len(train_dataloader) * epochs
    
    print(f"\nüéØ ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô:")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    print(f"   Training examples: {len(train_examples):,}")
    print(f"   Steps per epoch: {len(train_dataloader)}")
    print(f"   Total steps: {total_steps}")
    print(f"   Warmup steps: {warmup_steps}")
    print(f"   Device: {device.upper()}")
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡πÄ‡∏ß‡∏•‡∏≤
    if device == 'cuda':
        time_estimate = total_steps * 0.5 / 60
        print(f"\n‚ö° GPU Training - ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ ~{time_estimate:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ")
    elif device == 'mps':
        time_estimate = total_steps * 3.0 / 60
        print(f"\n‚ö° MPS Training - ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ ~{time_estimate:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ")
        print("   (batch_size=4 ‡∏à‡∏∞‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏´‡∏°‡∏î memory)")
    else:
        time_estimate = total_steps * 4.5 / 60
        print(f"\nüê¢ CPU Training - ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ ~{time_estimate:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ")
        print("   (Mac M2 ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 1-2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á output directory
    os.makedirs(output_path, exist_ok=True)
    
    # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
    print("\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô...\n")
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=epochs,
        warmup_steps=warmup_steps,
        output_path=output_path,
        show_progress_bar=True,
        save_best_model=True,
    )
    
    print(f"\n‚úÖ Fine-tuning ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
    print(f"üíæ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {output_path}")
    
    print("\nüìù ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:")
    print("   1. ‡∏£‡∏±‡∏ô: python generate_embeddings.py (‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)")
    print("   2. ‡∏£‡∏±‡∏ô: python app.py (‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà fine-tune ‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)")
    print("   3. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô!")
    
    return model


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("üé¨ Movie Recommendation Model Fine-tuning")
    print("=" * 80)
    
    # Fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 32GB RAM)
    finetuned_model = finetune_model(
        model_name='BAAI/bge-base-en-v1.5',  # ‡πÉ‡∏ä‡πâ SOTA model
        epochs=3,              # 3 epochs ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö fine-tuning
        batch_size=32,         # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 32 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 32GB RAM (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏Å!)
        warmup_steps=500,
        output_path='data/finetuned_model'
    )
    
    print("\n" + "=" * 80)
    print("üéâ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
    print("=" * 80)
